{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "08-test_driven_development.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wasilewskiJ/AMRencrypt/blob/main/content/08_test_driven_development/08-pytest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHtO18WB9y7P"
      },
      "source": [
        "\n",
        "<a href=\"https://colab.research.google.com/github/aviadr1/learn-advanced-python/blob/master/content/08_test_driven_development/08-pytest.ipynb\" target=\"_blank\">\n",
        "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
        "     title=\"Open this file in Google Colab\" alt=\"Colab\"/>\n",
        "</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awpj1eiRUuKM"
      },
      "source": [
        "# useful resources:\n",
        "1. https://stackabuse.com/test-driven-development-with-pytest/\n",
        "2. https://docs.pytest.org/en/latest/goodpractices.html#conventions-for-python-test-discovery\n",
        "3. https://blog.ionelmc.ro/2014/05/25/python-packaging/#the-structure\n",
        "4. https://github.com/vanzaj/tdd-pytest/blob/master/docs/tdd-pytest/content/tdd-basics.md\n",
        "5. https://opensource.com/article/18/6/pytest-plugins\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPIt7vgv97Vu"
      },
      "source": [
        "# setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd8QNG1J9y7U"
      },
      "source": [
        "1. install `pytest`\n",
        "2. install `pytest-sugar` which will give us nicer output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OAK0map9y7X"
      },
      "source": [
        "pip -q install pytest pytest-sugar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvUWrFRfV33t",
        "outputId": "d5ef9515-19e9-4306-ebd6-ecb5a1665d18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# move to tdd directory\n",
        "from pathlib import Path\n",
        "if Path.cwd().name != 'tdd':\n",
        "    %mkdir tdd\n",
        "    %cd tdd\n",
        "\n",
        "%pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/tdd/tdd'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrRF4Qh2W_Xk"
      },
      "source": [
        "# cleanup all files\n",
        "%rm *.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB1QY4sbPs4-"
      },
      "source": [
        "# How pytest discovers tests\n",
        "\n",
        "pytests uses the following [conventions](https://docs.pytest.org/en/latest/goodpractices.html#conventions-for-python-test-discovery) to automatically discovering tests:\n",
        "  1. files with tests should be called `test_*.py` or `*_test.py `\n",
        "  2. test function name should start with `test_`\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJjWla1qxd3i"
      },
      "source": [
        "# our first test\n",
        "to see if our code works, we can use the `assert` python keyword. pytest adds hooks to assertions to make them more useful"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9acpZigRVANF",
        "outputId": "fb8add6a-cc2c-4ed5-887d-97d51c8f094d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%file test_math.py\n",
        "\n",
        "import math\n",
        "def test_add():\n",
        "    assert 1+1 == 2\n",
        "\n",
        "def test_mul():\n",
        "    assert 6*7 == 42\n",
        "\n",
        "def test_sin():\n",
        "    assert math.sin(0) == 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test_math.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTBnZ3-vVe0p"
      },
      "source": [
        "now lets run pytest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXBQkMc8VeD8",
        "outputId": "b57ff406-b11d-4635-d2b1-a06827bea498",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        }
      },
      "source": [
        "!python -m pytest test_math.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mTest session starts (platform: linux, Python 3.6.9, pytest 5.3.5, pytest-sugar 0.9.2)\u001b[0m\n",
            "rootdir: /content\n",
            "plugins: sugar-0.9.2, xdist-1.31.0, forked-1.1.3, timeout-1.3.4\n",
            "\u001b[1mcollecting ... \u001b[0m\n",
            " \u001b[36m\u001b[0mtest_math.py\u001b[0m \u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m                                                \u001b[32m100% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m██\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m██\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m███\u001b[0m\n",
            "\n",
            "Results (0.02s):\n",
            "\u001b[32m       3 passed\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2x5SB05XNAF"
      },
      "source": [
        "Great! we just wrote 3 tests that shows that basic math still works\n",
        "\n",
        "Hurray!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKfU1dK3YMLQ"
      },
      "source": [
        "## your turn\n",
        "\n",
        "write a test for the following function.\n",
        "\n",
        "if there is a bug in the function, fix it\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nebAftmgbo-X",
        "outputId": "9e774bc9-b990-406c-a525-7bfb3259ca0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%file make_triangle.py\n",
        "\n",
        "# version 1\n",
        "\n",
        "def make_triangle(n):\n",
        "    \"\"\"\n",
        "    draws a triangle using '@' letters\n",
        "    for instance:\n",
        "        >>> print('\\n'.join(make_triangle(3))\n",
        "        @\n",
        "        @@\n",
        "        @@@\n",
        "    \"\"\"\n",
        "\n",
        "    for i in range(n):\n",
        "        yield '@' * i\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing make_triangle.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWIGFnRQcNUc"
      },
      "source": [
        "## solution\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWHxUDs3cYSr",
        "outputId": "cdc052ae-32ee-4275-8333-dab8d7512d7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%file test_make_triangle.py\n",
        "\n",
        "from make_triangle import make_triangle\n",
        "\n",
        "def test_make_triangle():\n",
        "    expected = \"@\"\n",
        "    actual = '\\n'.join(make_triangle(1))\n",
        "    assert actual == expected"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_make_triangle.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IInhHAiIc0ft",
        "outputId": "610e4a4a-99c3-4038-a7fb-a34d5595d24e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "source": [
        "!python -m pytest test_make_triangle.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mTest session starts (platform: linux, Python 3.6.9, pytest 5.3.5, pytest-sugar 0.9.2)\u001b[0m\n",
            "rootdir: /content\n",
            "plugins: sugar-0.9.2, xdist-1.31.0, forked-1.1.3, timeout-1.3.4\n",
            "\u001b[1mcollecting ... \u001b[0m\n",
            "\n",
            "―――――――――――――――――――――――――――――― test_make_triangle ――――――――――――――――――――――――――――――\n",
            "\n",
            "\u001b[1m    def test_make_triangle():\u001b[0m\n",
            "\u001b[1m        expected = \"@\"\u001b[0m\n",
            "\u001b[1m        actual = '\\n'.join(make_triangle(1))\u001b[0m\n",
            "\u001b[1m>       assert actual == expected\u001b[0m\n",
            "\u001b[1m\u001b[31mE       AssertionError: assert '' == '@'\u001b[0m\n",
            "\u001b[1m\u001b[31mE         + @\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_make_triangle.py\u001b[0m:7: AssertionError\n",
            "\n",
            " \u001b[36m\u001b[0mtest_make_triangle.py\u001b[0m \u001b[31m⨯\u001b[0m                                         \u001b[31m100% \u001b[0m\u001b[40m\u001b[31m█\u001b[0m\u001b[40m\u001b[31m█████████\u001b[0m\n",
            "\n",
            "Results (0.04s):\n",
            "\u001b[31m       1 failed\u001b[0m\n",
            "         - \u001b[36m\u001b[0mtest_make_triangle.py\u001b[0m:4 \u001b[31mtest_make_triangle\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmU6gxq2hc77"
      },
      "source": [
        "so the expected starts with `'@'` and the actual starts with `''` ...\n",
        "\n",
        "this is a bug! lets fix the code and re-run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opFtkK3WhtPZ",
        "outputId": "26a8bc29-99c1-44d6-8c3e-27ac78ba81ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%file make_triangle.py\n",
        "\n",
        "# version 2\n",
        "def make_triangle(n):\n",
        "    \"\"\"\n",
        "    draws a triangle using '@' letters\n",
        "    for instance:\n",
        "        >>> print('\\n'.join(make_triangle(3))\n",
        "        @\n",
        "        @@\n",
        "        @@@\n",
        "    \"\"\"\n",
        "\n",
        "    for i in range(1, n+1):\n",
        "        yield '@' * i"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting make_triangle.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nw_HYIjh2qh",
        "outputId": "8096fd15-0a38-4584-c8c3-41b74769db94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        }
      },
      "source": [
        "!python -m pytest test_make_triangle.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.6.9, pytest-3.6.4, py-1.8.1, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "collected 1 item                                                               \u001b[0m\u001b[1m\n",
            "\n",
            "test_make_triangle.py .\u001b[36m                                                  [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.01 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el2LYRdW0qZ6"
      },
      "source": [
        "# Pytest context-sensitive comparisons\n",
        "[Reference](https://docs.pytest.org/en/3.0.1/assert.html#making-use-of-context-sensitive-comparisons)\n",
        "\n",
        "pytest has rich support for providing context-sensitive information when it encounters comparisons.\n",
        "\n",
        "Special comparisons are done for a number of cases:\n",
        "- comparing long strings: a context diff is shown\n",
        "- comparing long sequences: first failing indices\n",
        "- comparing dicts: different entries\n",
        "\n",
        "Here's how this looks like for set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zC5qxb91HGL",
        "outputId": "4f96392c-5834-40b4-d3fd-764166044820",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%file test_compare_fruits.py\n",
        "def test_set_comparison():\n",
        "    set1 = set(['Apples', 'Bananas', 'Watermelon', 'Pear',  'Guave', 'Carambola', 'Plum'])\n",
        "    set2 = set(['Plum', 'Apples', 'Grapes', 'Watermelon','Pear', 'Guave', 'Carambola',  'Melon' ])\n",
        "    assert set1 == set2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test_compare_fruits.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcymuKrV11Pz",
        "outputId": "70300763-6636-4b3b-b36f-44b5f900876b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        }
      },
      "source": [
        "!python -m pytest test_compare_fruits.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mTest session starts (platform: linux, Python 3.6.9, pytest 5.3.5, pytest-sugar 0.9.2)\u001b[0m\n",
            "rootdir: /content\n",
            "plugins: sugar-0.9.2, xdist-1.31.0, forked-1.1.3, timeout-1.3.4\n",
            "\u001b[1mcollecting ... \u001b[0m\n",
            "\n",
            "――――――――――――――――――――――――――――― test_set_comparison ――――――――――――――――――――――――――――――\n",
            "\n",
            "\u001b[1m    def test_set_comparison():\u001b[0m\n",
            "\u001b[1m        set1 = set(['Apples', 'Bananas', 'Watermelon', 'Pear',  'Guave', 'Carambola', 'Plum'])\u001b[0m\n",
            "\u001b[1m        set2 = set(['Plum', 'Apples', 'Grapes', 'Watermelon','Pear', 'Guave', 'Carambola',  'Melon' ])\u001b[0m\n",
            "\u001b[1m>       assert set1 == set2\u001b[0m\n",
            "\u001b[1m\u001b[31mE       AssertionError: assert {'Apples', 'B..., 'Plum', ...} == {'Apples', 'C..., 'Pear', ...}\u001b[0m\n",
            "\u001b[1m\u001b[31mE         Extra items in the left set:\u001b[0m\n",
            "\u001b[1m\u001b[31mE         'Bananas'\u001b[0m\n",
            "\u001b[1m\u001b[31mE         Extra items in the right set:\u001b[0m\n",
            "\u001b[1m\u001b[31mE         'Melon'\u001b[0m\n",
            "\u001b[1m\u001b[31mE         'Grapes'\u001b[0m\n",
            "\u001b[1m\u001b[31mE         Use -v to get the full diff\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_compare_fruits.py\u001b[0m:4: AssertionError\n",
            "\n",
            " \u001b[36m\u001b[0mtest_compare_fruits.py\u001b[0m \u001b[31m⨯\u001b[0m                                        \u001b[31m100% \u001b[0m\u001b[40m\u001b[31m█\u001b[0m\u001b[40m\u001b[31m█████████\u001b[0m\n",
            "\n",
            "Results (0.03s):\n",
            "\u001b[31m       1 failed\u001b[0m\n",
            "         - \u001b[36m\u001b[0mtest_compare_fruits.py\u001b[0m:1 \u001b[31mtest_set_comparison\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8M63k6b6F12"
      },
      "source": [
        "## your turn\n",
        "\n",
        "test the following function `count_words()` and fix any bugs.\n",
        "\n",
        "the expected output from the function is given in `expected_output`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJRTUb1J9-t8"
      },
      "source": [
        "expected_output = {\n",
        " 'and': 2,\n",
        " 'chief': 2,\n",
        " 'didnt': 1,\n",
        " 'efficiency': 1,\n",
        " 'expected': 1,\n",
        " 'expects': 1,\n",
        " 'fear': 2,\n",
        " 'i': 1,\n",
        " 'inquisition': 2,\n",
        " 'is': 1,\n",
        " 'no': 1,\n",
        " 'one': 1,\n",
        " 'our': 1,\n",
        " 'ruthless': 1,\n",
        " 'spanish': 2,\n",
        " 'surprise': 3,\n",
        " 'the': 2,\n",
        " 'two': 1,\n",
        " 'weapon': 1,\n",
        " 'weapons': 1,\n",
        " 'well': 1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkSdtFl96HN-",
        "outputId": "b5e7d548-4f16-4cce-b4e0-febffdbff903",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%file spanish_inquisition.py\n",
        "# version 1: buggy\n",
        "import collections\n",
        "\n",
        "quote = \"\"\"\n",
        "Well, I didn't expected the Spanish Inquisition ...\n",
        "No one expects the Spanish Inquisition!\n",
        "Our chief weapon is surprise, fear and surprise;\n",
        "two chief weapons, fear, surprise, and ruthless efficiency!\n",
        "\"\"\"\n",
        "\n",
        "def remove_punctuation(quote):\n",
        "    quote.translate(str.maketrans('', '', \"',.!?;\")).lower()\n",
        "    return quote\n",
        "\n",
        "def count_words(quote):\n",
        "    quote = remove_punctuation(quote)\n",
        "    return dict(collections.Counter(quote.split(' ')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting spanish_inquisition.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwucG2PJ91xT"
      },
      "source": [
        "## solution\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOVhaPZu_J8f",
        "outputId": "a9f4496e-1ac6-473e-b4c1-355f70faf655",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%file test_spanish_inquisition.py\n",
        "\n",
        "from spanish_inquisition import *\n",
        "\n",
        "expected_output = {\n",
        " 'and': 2,\n",
        " 'chief': 2,\n",
        " 'didnt': 1,\n",
        " 'efficiency': 1,\n",
        " 'expected': 1,\n",
        " 'expects': 1,\n",
        " 'fear': 2,\n",
        " 'i': 1,\n",
        " 'inquisition': 2,\n",
        " 'is': 1,\n",
        " 'no': 1,\n",
        " 'one': 1,\n",
        " 'our': 1,\n",
        " 'ruthless': 1,\n",
        " 'spanish': 2,\n",
        " 'surprise': 3,\n",
        " 'the': 2,\n",
        " 'two': 1,\n",
        " 'weapon': 1,\n",
        " 'weapons': 1,\n",
        " 'well': 1}\n",
        "\n",
        "def test_spanish_inquisition():\n",
        "    actual = count_words(quote)\n",
        "    assert actual == expected_output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_spanish_inquisition.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZE9GzGA_hjW",
        "outputId": "e8fb06c1-484c-4043-9fc2-2c4236fbf8ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python -m pytest -vv test_spanish_inquisition.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mTest session starts (platform: linux, Python 3.6.9, pytest 5.3.5, pytest-sugar 0.9.2)\u001b[0m\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: sugar-0.9.2, xdist-1.31.0, forked-1.1.3, timeout-1.3.4\n",
            "\u001b[1mcollecting ... \u001b[0m\n",
            "\n",
            "――――――――――――――――――――――――――― test_spanish_inquisition ―――――――――――――――――――――――――――\n",
            "\n",
            "\u001b[1m    def test_spanish_inquisition():\u001b[0m\n",
            "\u001b[1m        actual = count_words(quote)\u001b[0m\n",
            "\u001b[1m>       assert actual == expected_output\u001b[0m\n",
            "\u001b[1m\u001b[31mE       assert {'\\n': 1,\\n '\\nWell,': 1,\\n '...\\nNo': 1,\\n 'I': 1,\\n 'Inquisition': 1,\\n 'Inquisition!\\nOur': 1,\\n 'Spanish': 2,\\n 'and': 2,\\n 'chief': 2,\\n \"didn't\": 1,\\n 'efficiency!': 1,\\n 'expected': 1,\\n 'expects': 1,\\n 'fear': 1,\\n 'fear,': 1,\\n 'is': 1,\\n 'one': 1,\\n 'ruthless': 1,\\n 'surprise,': 2,\\n 'surprise;\\ntwo': 1,\\n 'the': 2,\\n 'weapon': 1,\\n 'weapons,': 1} == {'and': 2,\\n 'chief': 2,\\n 'didnt': 1,\\n 'efficiency': 1,\\n 'expected': 1,\\n 'expects': 1,\\n 'fear': 2,\\n 'i': 1,\\n 'inquisition': 2,\\n 'is': 1,\\n 'no': 1,\\n 'one': 1,\\n 'our': 1,\\n 'ruthless': 1,\\n 'spanish': 2,\\n 'surprise': 3,\\n 'the': 2,\\n 'two': 1,\\n 'weapon': 1,\\n 'weapons': 1,\\n 'well': 1}\u001b[0m\n",
            "\u001b[1m\u001b[31mE         Common items:\u001b[0m\n",
            "\u001b[1m\u001b[31mE         {'and': 2,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          'chief': 2,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          'expected': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          'expects': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          'is': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          'one': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          'ruthless': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          'the': 2,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          'weapon': 1}\u001b[0m\n",
            "\u001b[1m\u001b[31mE         Differing items:\u001b[0m\n",
            "\u001b[1m\u001b[31mE         {'fear': 1} != {'fear': 2}\u001b[0m\n",
            "\u001b[1m\u001b[31mE         Left contains 13 more items:\u001b[0m\n",
            "\u001b[1m\u001b[31mE         {'\\n': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          '\\nWell,': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          '...\\nNo': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          'I': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          'Inquisition': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          'Inquisition!\\nOur': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          'Spanish': 2,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          \"didn't\": 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          'efficiency!': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          'fear,': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          'surprise,': 2,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          'surprise;\\ntwo': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          'weapons,': 1}\u001b[0m\n",
            "\u001b[1m\u001b[31mE         Right contains 11 more items:\u001b[0m\n",
            "\u001b[1m\u001b[31mE         {'didnt': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          'efficiency': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          'i': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          'inquisition': 2,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          'no': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          'our': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          'spanish': 2,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          'surprise': 3,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          'two': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          'weapons': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE          'well': 1}\u001b[0m\n",
            "\u001b[1m\u001b[31mE         Full diff:\u001b[0m\n",
            "\u001b[1m\u001b[31mE           {\u001b[0m\n",
            "\u001b[1m\u001b[31mE         -  '\\n': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE         -  '\\nWell,': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE         -  '...\\nNo': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE         -  'I': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE         -  'Inquisition': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE         -  'Inquisition!\\nOur': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE         -  'Spanish': 2,\u001b[0m\n",
            "\u001b[1m\u001b[31mE            'and': 2,\u001b[0m\n",
            "\u001b[1m\u001b[31mE            'chief': 2,\u001b[0m\n",
            "\u001b[1m\u001b[31mE         -  \"didn't\": 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE         ?  ^     --\u001b[0m\n",
            "\u001b[1m\u001b[31mE         +  'didnt': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE         ?  ^    +\u001b[0m\n",
            "\u001b[1m\u001b[31mE         -  'efficiency!': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE         ?             -\u001b[0m\n",
            "\u001b[1m\u001b[31mE         +  'efficiency': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE            'expected': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE            'expects': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE         -  'fear': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE         ?          ^\u001b[0m\n",
            "\u001b[1m\u001b[31mE         +  'fear': 2,\u001b[0m\n",
            "\u001b[1m\u001b[31mE         ?          ^\u001b[0m\n",
            "\u001b[1m\u001b[31mE         -  'fear,': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE         +  'i': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE         +  'inquisition': 2,\u001b[0m\n",
            "\u001b[1m\u001b[31mE            'is': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE         +  'no': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE            'one': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE         +  'our': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE            'ruthless': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE         +  'spanish': 2,\u001b[0m\n",
            "\u001b[1m\u001b[31mE         -  'surprise,': 2,\u001b[0m\n",
            "\u001b[1m\u001b[31mE         ?           -   ^\u001b[0m\n",
            "\u001b[1m\u001b[31mE         +  'surprise': 3,\u001b[0m\n",
            "\u001b[1m\u001b[31mE         ?              ^\u001b[0m\n",
            "\u001b[1m\u001b[31mE         -  'surprise;\\ntwo': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE            'the': 2,\u001b[0m\n",
            "\u001b[1m\u001b[31mE         +  'two': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE            'weapon': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE         -  'weapons,': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE         ?          -\u001b[0m\n",
            "\u001b[1m\u001b[31mE         +  'weapons': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE         +  'well': 1,\u001b[0m\n",
            "\u001b[1m\u001b[31mE           }\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_spanish_inquisition.py\u001b[0m:29: AssertionError\n",
            "\n",
            " \u001b[36mtest_spanish_inquisition.py\u001b[0m::test_spanish_inquisition\u001b[0m \u001b[31m⨯\u001b[0m         \u001b[31m100% \u001b[0m\u001b[40m\u001b[31m█\u001b[0m\u001b[40m\u001b[31m█████████\u001b[0m\n",
            "\n",
            "Results (0.04s):\n",
            "\u001b[31m       1 failed\u001b[0m\n",
            "         - \u001b[36m\u001b[0mtest_spanish_inquisition.py\u001b[0m:27 \u001b[31mtest_spanish_inquisition\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQjm-hif_98X",
        "outputId": "a749dd57-db52-4d42-ff08-8e6b5f8dab41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%file spanish_inquisition.py\n",
        "# version 2: fixed\n",
        "import collections\n",
        "\n",
        "quote = \"\"\"\n",
        "Well, I didn't expected the Spanish Inquisition ...\n",
        "No one expects the Spanish Inquisition!\n",
        "Our chief weapon is surprise, fear and surprise;\n",
        "two chief weapons, fear, surprise, and ruthless efficiency!\n",
        "\"\"\"\n",
        "\n",
        "def remove_punctuation(quote):\n",
        "    # quote.translate(str.maketrans('', '', \"',.!?;\")).lower() # BUG: missing return\n",
        "    return quote.translate(str.maketrans('', '', \"',.!?;\")).lower()\n",
        "\n",
        "def count_words(quote):\n",
        "    quote = remove_punctuation(quote)\n",
        "    # return dict(collections.Counter(quote.split(' '))) # BUG\n",
        "    return dict(collections.Counter(quote.split()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting spanish_inquisition.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI4G32m5AJ_P",
        "outputId": "fab1ee98-a611-443a-da83-2b2898f9442e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "!python -m pytest -vv test_spanish_inquisition.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mTest session starts (platform: linux, Python 3.6.9, pytest 5.3.5, pytest-sugar 0.9.2)\u001b[0m\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: sugar-0.9.2, xdist-1.31.0, forked-1.1.3, timeout-1.3.4\n",
            "\u001b[1mcollecting ... \u001b[0m\n",
            " \u001b[36mtest_spanish_inquisition.py\u001b[0m::test_spanish_inquisition\u001b[0m \u001b[32m✓\u001b[0m         \u001b[32m100% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█████████\u001b[0m\n",
            "\n",
            "Results (0.02s):\n",
            "\u001b[32m       1 passed\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v90fn0mbitTu"
      },
      "source": [
        "# Using fixtures to simplify tests\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1NkKG7PlQyk"
      },
      "source": [
        "## Motivating example\n",
        "\n",
        "Lets look at an example of class `Person`, where each person has a name and remembers their friends."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNw35je2i9iA",
        "outputId": "9c4d7ae0-236f-4c91-f5da-485cd3c37a9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%file person.py\n",
        "\n",
        "#version 1\n",
        "class Person:\n",
        "    def __init__(self, name, favorite_color, year_born):\n",
        "        self.name = name\n",
        "        self.favorite_color = favorite_color\n",
        "        self.year_born = year_born\n",
        "        self.friends = set()\n",
        "\n",
        "    def add_friend(self, other_person):\n",
        "        if not isinstance(other_person, Person): raise TypeError(other_person, 'is not a', Person)\n",
        "        self.friends.add(other_person)\n",
        "        other_person.friends.add(self)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'Person(name={self.name!r}, '  \\\n",
        "               f'favorite_color={self.favorite_color!r}, ' \\\n",
        "               f'year_born={self.year_born!r}, ' \\\n",
        "               f'friends={[f.name for f in self.friends]})'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting person.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqOAgl_olosJ"
      },
      "source": [
        "Lets write a test for `add_friend()` function.\n",
        "\n",
        "notice how the setup for the test is taking so much of the function, while also requiring _inventing_ a lot of repetitious data\n",
        "\n",
        "is there a way to reduce this boiler plate code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7kOQpl3j01h",
        "outputId": "07896c96-4249-4a1c-d72c-bfaf3d435cae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%file test_person.py\n",
        "\n",
        "from person import Person\n",
        "\n",
        "def test_name():\n",
        "    # setup\n",
        "    terry = Person(\n",
        "        'Terry Gilliam',\n",
        "        'red',\n",
        "        1940\n",
        "        )\n",
        "\n",
        "    # test\n",
        "    assert terry.name == 'Terry Gilliam'\n",
        "\n",
        "\n",
        "def test_add_friend():\n",
        "    # setup for the test\n",
        "    terry = Person(\n",
        "        'Terry Gilliam',\n",
        "        'red',\n",
        "        1940\n",
        "        )\n",
        "    eric = Person(\n",
        "        'Eric Idle',\n",
        "        'blue',\n",
        "        1943\n",
        "        )\n",
        "\n",
        "    # actual test\n",
        "    terry.add_friend(eric)\n",
        "    assert eric in terry.friends\n",
        "    assert terry in eric.friends"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_person.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYMxE55mjt02",
        "outputId": "58448a59-0222-420a-a981-ebc1a81fe952",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!python -m pytest -q test_person.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "..\u001b[36m                                                                       [100%]\u001b[0m\n",
            "\u001b[32m\u001b[1m2 passed in 0.01 seconds\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu8SxYwXp0n6"
      },
      "source": [
        "## Fixtures to the rescue\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G6Yj8hvq4hh"
      },
      "source": [
        "\n",
        "what is we had a magic factory that can conjure up a name, favorite color and birth year?\n",
        "\n",
        "then we could write our `test_name()` more simply like this:\n",
        "\n",
        "```python\n",
        "def test_name(person_name, favorite_color, birth_year):\n",
        "    person = Person(person_name, favorite_color, birth_year)\n",
        "    \n",
        "    # test\n",
        "    assert person.name == person_name\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJiA16D9q8Qc"
      },
      "source": [
        "furthermore, if we had a magic factory that can create `terry` and `eric` we could write our `test_add_friend()` function like this:\n",
        "\n",
        "```python\n",
        "def test_add_friend(eric, terry):\n",
        "    eric.add_friend(terry)\n",
        "    assert eric in terry.friends\n",
        "    assert terry in eric.friends\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmzljhcnrZii"
      },
      "source": [
        "fixtures in `pytest` allow us to create such magic factories using the `@pytest.fixture` notation.\n",
        "\n",
        "here's an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfC-AO6PqMIP",
        "outputId": "1d93cd1d-3cef-4e6a-e69f-5ab695205124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%file test_person_fixtures1.py\n",
        "\n",
        "import pytest\n",
        "from person import Person\n",
        "\n",
        "@pytest.fixture\n",
        "def person_name():\n",
        "    return 'Terry Gilliam'\n",
        "\n",
        "@pytest.fixture\n",
        "def birth_year():\n",
        "    return 1940\n",
        "\n",
        "@pytest.fixture\n",
        "def favorite_color():\n",
        "    return 'red'\n",
        "\n",
        "def test_person_name(person_name, favorite_color, birth_year):\n",
        "    person = Person(person_name, favorite_color, birth_year)\n",
        "\n",
        "    # test\n",
        "    assert person.name == person_name"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_person_fixtures1.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsg5u51AsS3B",
        "outputId": "d82c7d1d-c076-44cf-db7f-679b714b4532",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        }
      },
      "source": [
        "!python -m pytest test_person_fixtures1.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.6.9, pytest-3.6.4, py-1.8.1, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "collected 1 item                                                               \u001b[0m\u001b[1m\n",
            "\n",
            "test_person_fixtures1.py .\u001b[36m                                               [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.02 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4TIxQSBxYHo"
      },
      "source": [
        "what's happening here?\n",
        "\n",
        "`pytest` sees that the test function `test_person_name(person_name, favorite_color, birth_year)` requires three parameters, and searches for fixtures annotated with `@pytest.fixture` with the same name.\n",
        "\n",
        "when it finds them, it calls these fixtures on our behalf, and passes the return value as the parameter. in effect, it calls\n",
        "\n",
        "```python\n",
        "test_person_name(person_name=person_name(), favorite_color=favorite_color(), birth_year=birth_year()\n",
        "```\n",
        "\n",
        "note how much code this saves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itQIQgL1p1ub"
      },
      "source": [
        "## your turn\n",
        "1. rewrite the `test_add_friend` function to accept two parameters `def test_add_friend(eric, terry)`\n",
        "2. write fixtures for eric and terry\n",
        "3. run pytest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwO3ul3PuEI5"
      },
      "source": [
        "## solution\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgEClKykuFuM",
        "outputId": "069f166a-70a5-461c-d845-59981d540cc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%file test_person_fixtures2.py\n",
        "\n",
        "import pytest\n",
        "from person import Person\n",
        "\n",
        "@pytest.fixture\n",
        "def eric():\n",
        "    return Person('Eric Idle', 'red', 1943)\n",
        "\n",
        "@pytest.fixture\n",
        "def terry():\n",
        "    return Person('Terry Gilliam', 'blue', 1940)\n",
        "\n",
        "def test_add_friend(eric, terry):\n",
        "    eric.add_friend(terry)\n",
        "    assert eric in terry.friends\n",
        "    assert terry in eric.friends\n",
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test_person_fixtures2.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-OxB-hXu0Gh",
        "outputId": "94f228c4-1566-4ff7-d646-3a90ea5ef4f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!python -m pytest -q test_person_fixtures2.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\u001b[36m                                                                        [100%]\u001b[0m\n",
            "\u001b[32m\u001b[1m1 passed in 0.02 seconds\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTresMzCxjwc"
      },
      "source": [
        "# parameterizing fixtures\n",
        "\n",
        "Fixture functions can be parametrized in which case they will be called multiple times, each time executing the set of dependent tests, i. e. the tests that depend on this fixture.\n",
        "\n",
        "Test functions usually do not need to be aware of their re-running. Fixture parametrization helps to write exhaustive functional tests for components which themselves can be configured in multiple ways.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ucbc8ijzxy9q",
        "outputId": "b87d6ff0-bdb5-4f60-bee4-640f6b1c7f7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%file test_primes.py\n",
        "\n",
        "import pytest\n",
        "import math\n",
        "\n",
        "def is_prime(x):\n",
        "    return all(x % factor != 0 for factor in range(2, int(x/2)))\n",
        "\n",
        "@pytest.fixture(params=[2,3,5,7,11, 13, 17, 19, 101])\n",
        "def prime_number(request):\n",
        "    return request.param\n",
        "\n",
        "def test_prime(prime_number):\n",
        "    assert is_prime(prime_number) == True"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_primes.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OQjbA9T4XIv",
        "outputId": "34626dd4-4b6d-4d19-cd92-9038bb5ccb6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "source": [
        "!python -m pytest --verbose test_primes.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.6.9, pytest-3.6.4, py-1.8.1, pluggy-0.7.1 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "collected 9 items                                                              \u001b[0m\u001b[1m\n",
            "\n",
            "test_primes.py::test_prime[2] \u001b[32mPASSED\u001b[0m\u001b[36m                                     [ 11%]\u001b[0m\n",
            "test_primes.py::test_prime[3] \u001b[32mPASSED\u001b[0m\u001b[36m                                     [ 22%]\u001b[0m\n",
            "test_primes.py::test_prime[5] \u001b[32mPASSED\u001b[0m\u001b[36m                                     [ 33%]\u001b[0m\n",
            "test_primes.py::test_prime[7] \u001b[32mPASSED\u001b[0m\u001b[36m                                     [ 44%]\u001b[0m\n",
            "test_primes.py::test_prime[11] \u001b[32mPASSED\u001b[0m\u001b[36m                                    [ 55%]\u001b[0m\n",
            "test_primes.py::test_prime[13] \u001b[32mPASSED\u001b[0m\u001b[36m                                    [ 66%]\u001b[0m\n",
            "test_primes.py::test_prime[17] \u001b[32mPASSED\u001b[0m\u001b[36m                                    [ 77%]\u001b[0m\n",
            "test_primes.py::test_prime[19] \u001b[32mPASSED\u001b[0m\u001b[36m                                    [ 88%]\u001b[0m\n",
            "test_primes.py::test_prime[101] \u001b[32mPASSED\u001b[0m\u001b[36m                                   [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 9 passed in 0.03 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hYUoGSR3Ekp"
      },
      "source": [
        "## your turn\n",
        "\n",
        "test `is_prime()` for non prime numbers\n",
        "> bonus: can you find and fix the bug in `is_prime()` using a test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSgA7sba3QPp"
      },
      "source": [
        "## solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGRatrl_3Uw5",
        "outputId": "128b567b-6e71-4efd-efc4-a1920fe0141d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%file test_non_primes.py\n",
        "\n",
        "import pytest\n",
        "\n",
        "FIX_BUG = True\n",
        "if FIX_BUG:\n",
        "    def is_prime_fixed(x):\n",
        "        # notice the +1 - it is important when x=4\n",
        "        return all(x % factor != 0 for factor in range(2, int(x/2) + 1))\n",
        "    is_prime = is_prime_fixed\n",
        "else:\n",
        "    from test_primes import is_prime\n",
        "\n",
        "@pytest.fixture(params=[4, 6, 8, 9, 10, 12, 14, 15, 16, 28, 60, 100])\n",
        "def non_prime_number(request):\n",
        "    return request.param\n",
        "\n",
        "def test_non_primes(non_prime_number):\n",
        "    assert is_prime(non_prime_number) == False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_non_primes.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbDQVXfC3wyI",
        "outputId": "25375972-0b67-40aa-d957-940d6f1a7078",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "!python -m pytest --verbose test_non_primes.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.6.9, pytest-3.6.4, py-1.8.1, pluggy-0.7.1 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "collected 12 items                                                             \u001b[0m\u001b[1m\n",
            "\n",
            "test_non_primes.py::test_non_primes[4] \u001b[32mPASSED\u001b[0m\u001b[36m                            [  8%]\u001b[0m\n",
            "test_non_primes.py::test_non_primes[6] \u001b[32mPASSED\u001b[0m\u001b[36m                            [ 16%]\u001b[0m\n",
            "test_non_primes.py::test_non_primes[8] \u001b[32mPASSED\u001b[0m\u001b[36m                            [ 25%]\u001b[0m\n",
            "test_non_primes.py::test_non_primes[9] \u001b[32mPASSED\u001b[0m\u001b[36m                            [ 33%]\u001b[0m\n",
            "test_non_primes.py::test_non_primes[10] \u001b[32mPASSED\u001b[0m\u001b[36m                           [ 41%]\u001b[0m\n",
            "test_non_primes.py::test_non_primes[12] \u001b[32mPASSED\u001b[0m\u001b[36m                           [ 50%]\u001b[0m\n",
            "test_non_primes.py::test_non_primes[14] \u001b[32mPASSED\u001b[0m\u001b[36m                           [ 58%]\u001b[0m\n",
            "test_non_primes.py::test_non_primes[15] \u001b[32mPASSED\u001b[0m\u001b[36m                           [ 66%]\u001b[0m\n",
            "test_non_primes.py::test_non_primes[16] \u001b[32mPASSED\u001b[0m\u001b[36m                           [ 75%]\u001b[0m\n",
            "test_non_primes.py::test_non_primes[28] \u001b[32mPASSED\u001b[0m\u001b[36m                           [ 83%]\u001b[0m\n",
            "test_non_primes.py::test_non_primes[60] \u001b[32mPASSED\u001b[0m\u001b[36m                           [ 91%]\u001b[0m\n",
            "test_non_primes.py::test_non_primes[100] \u001b[32mPASSED\u001b[0m\u001b[36m                          [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m========================== 12 passed in 0.03 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO-EEPS32u5i",
        "outputId": "e9c8b4c9-eab8-4335-e933-33a940258bf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "all([factor for factor in range(2, int(4/2))])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayotSLStzjwx",
        "outputId": "4cabd563-a1d7-471d-a444-27e839060a26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        }
      },
      "source": [
        "!python -m pytest --verbose test_primes.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.6.9, pytest-3.6.4, py-1.8.1, pluggy-0.7.1 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "collected 21 items                                                             \u001b[0m\u001b[1m\n",
            "\n",
            "test_primes.py::test_prime[2] \u001b[32mPASSED\u001b[0m\u001b[36m                                     [  4%]\u001b[0m\n",
            "test_primes.py::test_prime[3] \u001b[32mPASSED\u001b[0m\u001b[36m                                     [  9%]\u001b[0m\n",
            "test_primes.py::test_prime[5] \u001b[32mPASSED\u001b[0m\u001b[36m                                     [ 14%]\u001b[0m\n",
            "test_primes.py::test_prime[7] \u001b[32mPASSED\u001b[0m\u001b[36m                                     [ 19%]\u001b[0m\n",
            "test_primes.py::test_prime[11] \u001b[32mPASSED\u001b[0m\u001b[36m                                    [ 23%]\u001b[0m\n",
            "test_primes.py::test_prime[13] \u001b[32mPASSED\u001b[0m\u001b[36m                                    [ 28%]\u001b[0m\n",
            "test_primes.py::test_prime[17] \u001b[32mPASSED\u001b[0m\u001b[36m                                    [ 33%]\u001b[0m\n",
            "test_primes.py::test_prime[19] \u001b[32mPASSED\u001b[0m\u001b[36m                                    [ 38%]\u001b[0m\n",
            "test_primes.py::test_prime[101] \u001b[32mPASSED\u001b[0m\u001b[36m                                   [ 42%]\u001b[0m\n",
            "test_primes.py::test_non_primes[4] \u001b[31mFAILED\u001b[0m\u001b[36m                                [ 47%]\u001b[0m\n",
            "test_primes.py::test_non_primes[6] \u001b[32mPASSED\u001b[0m\u001b[36m                                [ 52%]\u001b[0m\n",
            "test_primes.py::test_non_primes[8] \u001b[32mPASSED\u001b[0m\u001b[36m                                [ 57%]\u001b[0m\n",
            "test_primes.py::test_non_primes[9] \u001b[32mPASSED\u001b[0m\u001b[36m                                [ 61%]\u001b[0m\n",
            "test_primes.py::test_non_primes[10] \u001b[32mPASSED\u001b[0m\u001b[36m                               [ 66%]\u001b[0m\n",
            "test_primes.py::test_non_primes[12] \u001b[32mPASSED\u001b[0m\u001b[36m                               [ 71%]\u001b[0m\n",
            "test_primes.py::test_non_primes[14] \u001b[32mPASSED\u001b[0m\u001b[36m                               [ 76%]\u001b[0m\n",
            "test_primes.py::test_non_primes[15] \u001b[32mPASSED\u001b[0m\u001b[36m                               [ 80%]\u001b[0m\n",
            "test_primes.py::test_non_primes[16] \u001b[32mPASSED\u001b[0m\u001b[36m                               [ 85%]\u001b[0m\n",
            "test_primes.py::test_non_primes[28] \u001b[32mPASSED\u001b[0m\u001b[36m                               [ 90%]\u001b[0m\n",
            "test_primes.py::test_non_primes[60] \u001b[32mPASSED\u001b[0m\u001b[36m                               [ 95%]\u001b[0m\n",
            "test_primes.py::test_non_primes[100] \u001b[32mPASSED\u001b[0m\u001b[36m                              [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[31m\u001b[1m______________________________ test_non_primes[4] ______________________________\u001b[0m\n",
            "\n",
            "non_prime_number = 4\n",
            "\n",
            "\u001b[1m    def test_non_primes(non_prime_number):\u001b[0m\n",
            "\u001b[1m>       assert is_prime(non_prime_number) == False\u001b[0m\n",
            "\u001b[1m\u001b[31mE       assert True == False\u001b[0m\n",
            "\u001b[1m\u001b[31mE        +  where True = is_prime(4)\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_primes.py\u001b[0m:20: AssertionError\n",
            "\u001b[31m\u001b[1m===================== 1 failed, 20 passed in 0.06 seconds ======================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HkdBwuXB1QU"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiaLU-XY5NbQ"
      },
      "source": [
        "# printing and logging within tests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GKZD-Ek-_ym"
      },
      "source": [
        "## printing\n",
        "[Reference](https://docs.pytest.org/en/latest/capture.html)\n",
        "\n",
        "You can use prints within tests to provide additional debug info.\n",
        "\n",
        "pytest redirects the output and captured the output of each test. it then:\n",
        "- __suppresses__ the output of all __successful__ tests (for brevity)\n",
        "- __shows__ the output off all __failed__ tests (for debugging)\n",
        "- both `stdout` and `stderr` are captured\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXhnkzC-0mM9",
        "outputId": "34812b5c-ff00-4e81-a6a6-26a8ec306c19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%file test_prints.py\n",
        "import sys\n",
        "\n",
        "def test_print_success():\n",
        "    print(\n",
        "        \"\"\"\n",
        "        @@@@@@@@@@@@@@@\n",
        "        this statement will NOT be printed\n",
        "        @@@@@@@@@@@@@@@\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    assert 6*7 == 42\n",
        "\n",
        "def test_print_fail():\n",
        "\n",
        "    print(\n",
        "        \"\"\"\n",
        "        @@@@@@@@@@@@@@@\n",
        "        this statement WILL be printed\n",
        "        @@@@@@@@@@@@@@@\n",
        "        \"\"\"\n",
        "    )\n",
        "    assert True == False\n",
        "\n",
        "\n",
        "def test_stderr_capture_success():\n",
        "    print(\n",
        "        \"\"\"\n",
        "        @@@@@@@@@@@@@@@\n",
        "        this STDERR statement will NOT be printed\n",
        "        @@@@@@@@@@@@@@@\n",
        "        \"\"\",\n",
        "        file=sys.stderr\n",
        "    )\n",
        "\n",
        "    assert True\n",
        "\n",
        "\n",
        "def test_stderr_capture_fail():\n",
        "    print(\n",
        "        \"\"\"\n",
        "        @@@@@@@@@@@@@@@\n",
        "        this STDERR statement WILL be printed\n",
        "        @@@@@@@@@@@@@@@\n",
        "        \"\"\",\n",
        "        file=sys.stderr\n",
        "    )\n",
        "\n",
        "    assert False\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_prints.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCnf85Ba6lDZ",
        "outputId": "61b69b5e-3911-4726-c356-56e400466e68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 812
        }
      },
      "source": [
        "!python -m pytest -q test_prints.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".F.F\u001b[36m                                                                     [100%]\u001b[0m\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[31m\u001b[1m_______________________________ test_print_fail ________________________________\u001b[0m\n",
            "\n",
            "\u001b[1m    def test_print_fail():\u001b[0m\n",
            "\u001b[1m    \u001b[0m\n",
            "\u001b[1m        print(\u001b[0m\n",
            "\u001b[1m            \"\"\"\u001b[0m\n",
            "\u001b[1m            @@@@@@@@@@@@@@@\u001b[0m\n",
            "\u001b[1m            this statement WILL be printed\u001b[0m\n",
            "\u001b[1m            @@@@@@@@@@@@@@@\u001b[0m\n",
            "\u001b[1m            \"\"\"\u001b[0m\n",
            "\u001b[1m        )\u001b[0m\n",
            "\u001b[1m>       assert True == False\u001b[0m\n",
            "\u001b[1m\u001b[31mE       assert True == False\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_prints.py\u001b[0m:23: AssertionError\n",
            "----------------------------- Captured stdout call -----------------------------\n",
            "\n",
            "        @@@@@@@@@@@@@@@\n",
            "        this statement WILL be printed\n",
            "        @@@@@@@@@@@@@@@\n",
            "        \n",
            "\u001b[31m\u001b[1m___________________________ test_stderr_capture_fail ___________________________\u001b[0m\n",
            "\n",
            "\u001b[1m    def test_stderr_capture_fail():\u001b[0m\n",
            "\u001b[1m        print(\u001b[0m\n",
            "\u001b[1m            \"\"\"\u001b[0m\n",
            "\u001b[1m            @@@@@@@@@@@@@@@\u001b[0m\n",
            "\u001b[1m            this STDERR statement WILL be printed\u001b[0m\n",
            "\u001b[1m            @@@@@@@@@@@@@@@\u001b[0m\n",
            "\u001b[1m            \"\"\",\u001b[0m\n",
            "\u001b[1m            file=sys.stderr\u001b[0m\n",
            "\u001b[1m        )\u001b[0m\n",
            "\u001b[1m    \u001b[0m\n",
            "\u001b[1m>       assert False\u001b[0m\n",
            "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_prints.py\u001b[0m:49: AssertionError\n",
            "----------------------------- Captured stderr call -----------------------------\n",
            "\n",
            "        @@@@@@@@@@@@@@@\n",
            "        this STDERR statement WILL be printed\n",
            "        @@@@@@@@@@@@@@@\n",
            "        \n",
            "\u001b[31m\u001b[1m2 failed, 2 passed in 0.04 seconds\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVObE-xb_PlT"
      },
      "source": [
        "## logging\n",
        "[Reference](https://docs.pytest.org/en/latest/logging.html)\n",
        "\n",
        "pytest captures log messages of level WARNING or above automatically and displays them in their own section for each failed test in the same manner as captured stdout and stderr.\n",
        "\n",
        "- WARNING and above will displayed for failed tests\n",
        "- INFO and below will not be displayed\n",
        "\n",
        "example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScR82eTM_iv1",
        "outputId": "56c4efbf-447b-4c88-da71-2503feacdc3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%file test_logging.py\n",
        "\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def test_logging_warning_success():\n",
        "    logger.warning('\\n\\n @@@ this will NOT be printed \\n\\n')\n",
        "    assert True\n",
        "\n",
        "def test_logging_warning_fail():\n",
        "    logger.warning('\\n\\n @@@ this WILL be printed @@@ \\n\\n')\n",
        "    assert False\n",
        "\n",
        "def test_logging_info_fail():\n",
        "    logger.info('\\n\\n @@@ this will NOT be printed @@@ \\n\\n')\n",
        "    assert False\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_logging.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0AveLPyAHR3",
        "outputId": "a1b23c00-aaf8-4697-b69e-0eabb0a5cd69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        }
      },
      "source": [
        "!python -m pytest test_logging.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.6.9, pytest-3.6.4, py-1.8.1, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "collected 3 items                                                              \u001b[0m\u001b[1m\n",
            "\n",
            "test_logging.py .FF\u001b[36m                                                      [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[31m\u001b[1m__________________________ test_logging_warning_fail ___________________________\u001b[0m\n",
            "\n",
            "\u001b[1m    def test_logging_warning_fail():\u001b[0m\n",
            "\u001b[1m        logger.warning('\\n\\n @@@ this WILL be printed @@@ \\n\\n')\u001b[0m\n",
            "\u001b[1m>       assert False\u001b[0m\n",
            "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_logging.py\u001b[0m:12: AssertionError\n",
            "------------------------------ Captured log call -------------------------------\n",
            "test_logging.py             11 WARNING  \n",
            "\n",
            " @@@ this WILL be printed @@@\n",
            "\u001b[31m\u001b[1m____________________________ test_logging_info_fail ____________________________\u001b[0m\n",
            "\n",
            "\u001b[1m    def test_logging_info_fail():\u001b[0m\n",
            "\u001b[1m        logger.info('\\n\\n @@@ this will NOT be printed @@@ \\n\\n')\u001b[0m\n",
            "\u001b[1m>       assert False\u001b[0m\n",
            "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_logging.py\u001b[0m:16: AssertionError\n",
            "\u001b[31m\u001b[1m====================== 2 failed, 1 passed in 0.04 seconds ======================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxVCJnUY5I4N"
      },
      "source": [
        "## your turn\n",
        "\n",
        "We give below an implementation of the _FizzBuzz_ puzzle:\n",
        "> Write a function that returns the numbers from 1 to 100. But for multiples of three returns “Fizz” instead of the number and for the multiples of five returns “Buzz”. For numbers which are multiples of both three and five return “FizzBuzz”.\n",
        "\n",
        "thus this SHOULD be true\n",
        "```python\n",
        ">>> fizzbuzz() # should return the following (abridged) output\n",
        "[1, 2, 'Fizz', 4, 'Buzz', 6, 7, 8, 'Fizz', 'Buzz', 11, 'Fizz', 13, 14, 'FizzBuzz', ... ]\n",
        "```\n",
        "\n",
        "BUT the implementation is buggy. can you write tests for it and fix it?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9DEbsHGBsUj",
        "outputId": "756ba06c-c188-4fe7-8e76-c97560836ea6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "%%file fizzbuzz.py\n",
        "\n",
        "def is_multiple(n, divisor):\n",
        "    return n % divisor == 0\n",
        "\n",
        "def fizzbuzz():\n",
        "    \"\"\"\n",
        "    expected output: list with elements numbers\n",
        "        [1, 2, 'Fizz', 4, 'Buzz', 'Fizz', 7, 8, 'Fizz', 'Buzz', 11, 'Fizz', 13, 14, 'FizzBuzz', ... ]\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    for i in range(100):\n",
        "        if is_multiple(i, 3):\n",
        "            return \"Fizz\"\n",
        "        elif is_multiple(i, 5):\n",
        "            return \"Buzz\"\n",
        "        elif is_multiple(i, 3) and is_multiple(i, 5):\n",
        "            return \"FizzBuzz\"\n",
        "        else:\n",
        "            return i\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing fizzbuzz.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCpyi39tDtOL"
      },
      "source": [
        "## solution\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44bJMzRsD0j2",
        "outputId": "50c454f1-4e11-4564-d12b-3e3878fa757c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%file test_fizzbuzz.py\n",
        "\n",
        "FIX_BUG = 1\n",
        "if not FIX_BUG:\n",
        "    from fizzbuzz import fizzbuzz\n",
        "else:\n",
        "    def fizzbuzz_fixed():\n",
        "        def translate(i):\n",
        "            if i%3 == 0 and i%5 == 0:\n",
        "                return \"FizzBuzz\"\n",
        "            elif i%3 == 0:\n",
        "                return \"Fizz\"\n",
        "            elif i%5 == 0:\n",
        "                return \"Buzz\"\n",
        "            else:\n",
        "                return i\n",
        "\n",
        "        return [translate(i) for i in range(1, 100+1)]\n",
        "\n",
        "    fizzbuzz = fizzbuzz_fixed\n",
        "\n",
        "\n",
        "import pytest\n",
        "@pytest.fixture\n",
        "def fizzbuzz_result():\n",
        "    result = fizzbuzz()\n",
        "    print(result)\n",
        "    return result\n",
        "\n",
        "@pytest.fixture\n",
        "def fizzbuzz_dict(fizzbuzz_result):\n",
        "    return dict(enumerate(fizzbuzz_result, 1))\n",
        "\n",
        "def test_fizzbuzz_len(fizzbuzz_result):\n",
        "    assert len(fizzbuzz_result) == 100\n",
        "\n",
        "def test_fizzbuzz_len(fizzbuzz_result):\n",
        "    assert type(fizzbuzz_result) == list\n",
        "\n",
        "def test_fizzbuzz_first_element(fizzbuzz_dict):\n",
        "    assert fizzbuzz_dict[1] == 1\n",
        "\n",
        "def test_fizzbuzz_3(fizzbuzz_dict):\n",
        "    assert fizzbuzz_dict[3] == 'Fizz'\n",
        "\n",
        "def test_fizzbuzz_5(fizzbuzz_dict):\n",
        "    assert fizzbuzz_dict[5] == 'Buzz'\n",
        "\n",
        "def test_fizzbuzz_15(fizzbuzz_dict):\n",
        "    assert fizzbuzz_dict[15] == 'FizzBuzz'\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_fizzbuzz.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Twogbse2EQvo",
        "outputId": "3fe68876-056f-4ca5-8b1b-5fcbc29b76a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        }
      },
      "source": [
        "!python -m pytest test_fizzbuzz.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.6.9, pytest-3.6.4, py-1.8.1, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "collected 5 items                                                              \u001b[0m\u001b[1m\n",
            "\n",
            "test_fizzbuzz.py .....\u001b[36m                                                   [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 5 passed in 0.03 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOyVKYiRLi1Q"
      },
      "source": [
        "# float: when things are (almost) equal\n",
        "[Reference](https://docs.pytest.org/en/latest/reference.html#pytest-approx)\n",
        "\n",
        "consider the following code, what do you expect the result to be?\n",
        "```\n",
        "x = 0.1 + 0.2\n",
        "y = 0.3\n",
        "print('x == y', x ==y) # what will it print?\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKLgutHfMH-U",
        "outputId": "cea086a1-339a-43c7-de0a-7f216f94a840",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = 0.1 + 0.2\n",
        "y = 0.3\n",
        "print('x == y:', x == y) # what will it print?"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x == y: False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGNsC4VeLlMJ"
      },
      "source": [
        "if you had anticipated `True` it means you haven't tried testing code with `float` data yet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOj89Un8MZZz",
        "outputId": "089c06a9-8725-46cd-9c32-bafea0b97840",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(x, '!=', y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.30000000000000004 != 0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFI1LGmyMpXZ"
      },
      "source": [
        "the issue is that float is _approxiamtely_ accurate (enough for most calculations) but may have small rounding errors.\n",
        "\n",
        "here'e a common but ugly way to test for float equivalence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D3uxeokNRxi",
        "outputId": "2dd2ccdf-8c41-40b2-dc45-fdf5c09c1c8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "abs((0.1 + 0.2) - 0.3) < 1e-6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3NPogWSNanZ"
      },
      "source": [
        "here's a more pythonic and pytest-tic way, using `pytest.approx`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BApEzps0NZ9Y",
        "outputId": "258afdc5-851e-4d2c-a1e7-68fe1cde7dd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from pytest import approx\n",
        "0.1 + 0.2 == approx(0.3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tgiv7X9QNt4G"
      },
      "source": [
        "## your turn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhtvMq5gPCO7"
      },
      "source": [
        "\n",
        "test that\n",
        "- `math.sin(0) == 0`,\n",
        "- `math.sin(math.pi / 2) == 1`\n",
        "- `math.sin(math.pi) == 0`\n",
        "- `math.sin(math.pi * 3/2) == -1`\n",
        "- `math.sin(math.pi * 2) == 0`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4vZMr7COM7Z"
      },
      "source": [
        "## solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOb7g8BhOO44",
        "outputId": "1bcfabd0-a5ef-4399-f66a-58fbc06cbdbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%file test_sin.py\n",
        "\n",
        "from pytest import approx\n",
        "import math\n",
        "def test_sin():\n",
        "    assert math.sin(0) == 0\n",
        "    assert math.sin(math.pi / 2) == 1\n",
        "    assert math.sin(math.pi) == approx(0)\n",
        "    assert math.sin(math.pi * 3/2) == approx(-1)\n",
        "    assert math.sin(math.pi * 2) == approx(0)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_sin.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLWreR5DOs__",
        "outputId": "b32a0521-fffd-402c-c472-35971f2b35b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "source": [
        "!python -m pytest test_sin.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.6.9, pytest-3.6.4, py-1.8.1, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "collected 1 item                                                               \u001b[0m\u001b[1m\n",
            "\n",
            "test_sin.py F\u001b[36m                                                            [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[31m\u001b[1m___________________________________ test_sin ___________________________________\u001b[0m\n",
            "\n",
            "\u001b[1m    def test_sin():\u001b[0m\n",
            "\u001b[1m        assert math.sin(0) == 0\u001b[0m\n",
            "\u001b[1m        assert math.sin(math.pi / 2) == 1\u001b[0m\n",
            "\u001b[1m>       assert math.sin(math.pi) == 0 #approx(0)\u001b[0m\n",
            "\u001b[1m\u001b[31mE       assert 1.2246467991473532e-16 == 0\u001b[0m\n",
            "\u001b[1m\u001b[31mE        +  where 1.2246467991473532e-16 = <built-in function sin>(3.141592653589793)\u001b[0m\n",
            "\u001b[1m\u001b[31mE        +    where <built-in function sin> = math.sin\u001b[0m\n",
            "\u001b[1m\u001b[31mE        +    and   3.141592653589793 = math.pi\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_sin.py\u001b[0m:7: AssertionError\n",
            "\u001b[31m\u001b[1m=========================== 1 failed in 0.03 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC62pY7IlKBx"
      },
      "source": [
        "# adding timeouts to tests\n",
        "[Reference](https://pypi.org/project/pytest-timeout/)\n",
        "\n",
        "Sometimes code gets stuck in an infinite loop, or waiting for a response from a server.\n",
        "Sometimes, tests that run too long is in _itself_ an indication of failure.\n",
        "\n",
        "how can we add timeouts to tests to avoid getting stuck?\n",
        "the package `pytest-timeout` solves for that by providing a plugin to pytest.\n",
        "\n",
        "1. install the package using `pip install pytest-timeout`\n",
        "2. you can set timeouts individually on tests by marking them with the `@pytest.mark.timeout(timeout=60)` decorator\n",
        "3. you can set the timeout for all tests globally by using the timeout commandline parameter for pytest, like so:`pytest --timeout=300`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loRinVVZlaUe"
      },
      "source": [
        "pip install -q pytest-timeout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr_kMCnLlOiT",
        "outputId": "fc82afd9-f14d-4d9f-ea16-cf361e12e5c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%file test_timeouts.py\n",
        "\n",
        "import pytest\n",
        "\n",
        "@pytest.mark.timeout(5)\n",
        "def test_infinite_sleep():\n",
        "    import time\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "        print('sleeping ...')\n",
        "\n",
        "def test_empty():\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_timeouts.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikU7sTVUl7dV",
        "outputId": "ae1e4ff9-94b3-4bae-be4c-06e00b32a882",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        }
      },
      "source": [
        "!python -m pytest --verbose test_timeouts.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mTest session starts (platform: linux, Python 3.6.9, pytest 5.3.5, pytest-sugar 0.9.2)\u001b[0m\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: sugar-0.9.2, xdist-1.31.0, forked-1.1.3, timeout-1.3.4\n",
            "\u001b[1mcollecting ... \u001b[0m\n",
            "\n",
            "――――――――――――――――――――――――――――― test_infinite_sleep ――――――――――――――――――――――――――――――\n",
            "\n",
            "\u001b[1m    @pytest.mark.timeout(5)\u001b[0m\n",
            "\u001b[1m    def test_infinite_sleep():\u001b[0m\n",
            "\u001b[1m        import time\u001b[0m\n",
            "\u001b[1m        while True:\u001b[0m\n",
            "\u001b[1m>           time.sleep(1)\u001b[0m\n",
            "\u001b[1m\u001b[31mE           Failed: Timeout >5.0s\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_timeouts.py\u001b[0m:8: Failed\n",
            "----------------------------- Captured stdout call -----------------------------\n",
            "sleeping ...\n",
            "sleeping ...\n",
            "sleeping ...\n",
            "sleeping ...\n",
            "\n",
            " \u001b[36mtest_timeouts.py\u001b[0m::test_infinite_sleep\u001b[0m \u001b[31m⨯\u001b[0m                          \u001b[31m50% \u001b[0m\u001b[40m\u001b[31m█\u001b[0m\u001b[40m\u001b[31m████     \u001b[0m\n",
            " \u001b[36mtest_timeouts.py\u001b[0m::test_empty\u001b[0m \u001b[32m✓\u001b[0m                                  \u001b[31m100% \u001b[0m\u001b[40m\u001b[31m█\u001b[0m\u001b[40m\u001b[31m████\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m████\u001b[0m\n",
            "\n",
            "Results (5.03s):\n",
            "\u001b[32m       1 passed\u001b[0m\n",
            "\u001b[31m       1 failed\u001b[0m\n",
            "         - \u001b[36m\u001b[0mtest_timeouts.py\u001b[0m:4 \u001b[31mtest_infinite_sleep\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHB2vqMln7mJ"
      },
      "source": [
        "notice how the `test_empty` test still runs and passes, even though the previous test was aborted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8hls8GrqTwp"
      },
      "source": [
        "## your turn\n",
        "\n",
        "1. use the `requests` module to `.get()` the url http://httpstat.us/101 and call `.raise_for_status()`\n",
        "2. since this will hang forever, use a timeout on the test so that it fails after 5 seconds\n",
        "3. since the test is guranteed to fail, mark it with the `xfail` (_expected fail_) annotation `@pytest.mark.xfail(reason='timeout')`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPBfhTmnqoay",
        "outputId": "c4e661bf-3f9e-4137-f780-6affcac49b54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%file test_http101_timeout.py\n",
        "\n",
        "import pytest\n",
        "import requests\n",
        "\n",
        "@pytest.mark.xfail(reason='timeout')\n",
        "@pytest.mark.timeout(2)\n",
        "def test_http101_timeout():\n",
        "    response = requests.get('http://httpstat.us/101')\n",
        "    response.raise_for_status()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_http101_timeout.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Muj2guSLrDDf",
        "outputId": "11f7721b-b174-41ad-82d0-4954e9e22f80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        }
      },
      "source": [
        "!python -m pytest test_http101_timeout.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mTest session starts (platform: linux, Python 3.6.9, pytest 5.3.5, pytest-sugar 0.9.2)\u001b[0m\n",
            "rootdir: /content\n",
            "plugins: sugar-0.9.2, xdist-1.31.0, forked-1.1.3, timeout-1.3.4\n",
            "\u001b[1mcollecting ... \u001b[0m\n",
            " \u001b[36m\u001b[0mtest_http101_timeout.py\u001b[0m \u001b[32mx\u001b[0m                                       \u001b[32m100% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█████████\u001b[0m\n",
            "\n",
            "Results (5.22s):\n",
            "\u001b[32m       1 xfailed\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld15jWJZcZ1A"
      },
      "source": [
        "# testing for exceptions\n",
        "[Reference](https://docs.pytest.org/en/3.0.1/assert.html#assertions-about-expected-exceptions)\n",
        "\n",
        "consider the following code fragment from `person.py`:\n",
        "\n",
        "```python\n",
        "class Person:\n",
        "    def add_friend(self, other_person):\n",
        "        if not isinstance(other_person, Person) raise TypeError(other_person, 'is not a', Person)\n",
        "        self.friends.add(other_person)\n",
        "        other_person.friends.add(self)\n",
        "```\n",
        "\n",
        "the `add_friend()` method will raise an exception if it is used with a parameter which is not a `Person`\n",
        "\n",
        "how can we test this?\n",
        "\n",
        "if we wrap the code that is supposed to throw the exc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_JMpDtFfOlO",
        "outputId": "3d0e65c5-973a-47e5-96f8-a7ce0705b317",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%file test_add_person_exception.py\n",
        "\n",
        "from person import Person\n",
        "from test_person_fixtures2 import *\n",
        "\n",
        "def test_add_person_exception(terry):\n",
        "    with pytest.raises(TypeError):\n",
        "        terry.add_friend(\"a shrubbey!\")\n",
        "\n",
        "def test_add_person_exception_detailed(terry):\n",
        "    with pytest.raises(TypeError) as excinfo:\n",
        "        terry.add_friend(\"a shrubbey!\")\n",
        "\n",
        "    assert 'Person' in str(excinfo.value)\n",
        "\n",
        "@pytest.mark.xfail(reason='expected to fail')\n",
        "def test_add_person_no_exception(terry, eric):\n",
        "    with pytest.raises(TypeError): # is expecting an exception that won't happen\n",
        "        terry.add_friend(eric) # this does not throw an exception\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_add_person_exception.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7V9YlP82f33w",
        "outputId": "6a47fedb-ed06-4f90-853e-b590c4faea79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "!python -m pytest test_add_person_exception.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mTest session starts (platform: linux, Python 3.6.9, pytest 5.3.5, pytest-sugar 0.9.2)\u001b[0m\n",
            "rootdir: /content\n",
            "plugins: sugar-0.9.2, xdist-1.31.0, forked-1.1.3, timeout-1.3.4\n",
            "\u001b[1mcollecting ... \u001b[0m\n",
            " \u001b[36m\u001b[0mtest_add_person_exception.py\u001b[0m \u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32mx\u001b[0m\u001b[32m✓\u001b[0m                               \u001b[32m100% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m██\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m██\u001b[0m\n",
            "\n",
            "Results (0.04s):\n",
            "\u001b[32m       3 passed\u001b[0m\n",
            "\u001b[32m       1 xfailed\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPuuwkvRiZX-"
      },
      "source": [
        "## your turn\n",
        "use the `requests` module and the `.raise_for_status()` method\n",
        "\n",
        "1. test that `.raise_for_status` will raise an exception when accessing the following URLs:\n",
        "   - http://httpstat.us/401\n",
        "   - http://httpstat.us/404\n",
        "   - http://httpstat.us/500\n",
        "   - http://httpstat.us/501\n",
        "2. test that `.raise_for_status` will NOT raise an exception when accessing the following URLs:\n",
        "   - http://httpstat.us/200\n",
        "   - http://httpstat.us/201\n",
        "   - http://httpstat.us/202\n",
        "   - http://httpstat.us/203\n",
        "   - http://httpstat.us/204\n",
        "   - http://httpstat.us/303\n",
        "   - http://httpstat.us/304  \n",
        "\n",
        "### hints:\n",
        "1. the `requests` module raises exceptions of type `requests.HTTPError`\n",
        "1. use parameterized fixtures to avoid writing a lot of tests or boilerplate code\n",
        "2. use timeouts to avoid tests that wait forever\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A692WcjLuLWe"
      },
      "source": [
        "## solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wX1Rfn7jw5u",
        "outputId": "91d8ec3f-ff8d-48be-b0b3-895424c8497c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%file test_requests.py\n",
        "\n",
        "import pytest\n",
        "import requests\n",
        "\n",
        "@pytest.fixture(params=[200, 201, 202, 203, 204, 303, 304])\n",
        "def good_url(request):\n",
        "    return f'http://httpstat.us/{request.param}'\n",
        "\n",
        "@pytest.fixture(params=[401, 404, 500, 501])\n",
        "def bad_url(request):\n",
        "    return f'http://httpstat.us/{request.param}'\n",
        "\n",
        "@pytest.mark.timeout(2)\n",
        "def test_good_urls(good_url):\n",
        "    response = requests.get(good_url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "@pytest.mark.timeout(2)\n",
        "def test_bad_urls(bad_url):\n",
        "    response = requests.get(bad_url)\n",
        "    with pytest.raises(requests.HTTPError):\n",
        "        response.raise_for_status()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_requests.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O586GEYpzR98",
        "outputId": "f18df864-7d47-4473-ebe8-490a6614961e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "source": [
        "pip install pytest-sugar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytest-sugar\n",
            "  Downloading https://files.pythonhosted.org/packages/da/3b/f1e3c8830860c1df8f0e0f6713932475141210cfa021e362ca2774d2bf02/pytest_sugar-0.9.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: packaging>=14.1 in /usr/local/lib/python3.6/dist-packages (from pytest-sugar) (20.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from pytest-sugar) (1.1.0)\n",
            "Requirement already satisfied: pytest>=2.9 in /usr/local/lib/python3.6/dist-packages (from pytest-sugar) (5.3.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=14.1->pytest-sugar) (2.4.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging>=14.1->pytest-sugar) (1.12.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=2.9->pytest-sugar) (8.2.0)\n",
            "Requirement already satisfied: pluggy<1.0,>=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest>=2.9->pytest-sugar) (0.13.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest>=2.9->pytest-sugar) (0.1.8)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=2.9->pytest-sugar) (19.3.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=2.9->pytest-sugar) (1.8.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=2.9->pytest-sugar) (1.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=2.9->pytest-sugar) (2.2.0)\n",
            "Installing collected packages: pytest-sugar\n",
            "Successfully installed pytest-sugar-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KdmZUkWktL0",
        "outputId": "ccaed18d-88b9-40c4-deb8-8e323cbf7064",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "!python -m pytest --verbose test_requests.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mTest session starts (platform: linux, Python 3.6.9, pytest 5.3.5, pytest-sugar 0.9.2)\u001b[0m\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: sugar-0.9.2, xdist-1.31.0, forked-1.1.3, timeout-1.3.4\n",
            "\u001b[1mcollecting ... \u001b[0m\n",
            " \u001b[36mtest_requests.py\u001b[0m::test_good_urls[200]\u001b[0m \u001b[32m✓\u001b[0m                           \u001b[32m9% \u001b[0m\u001b[40m\u001b[32m▉\u001b[0m\u001b[40m\u001b[32m         \u001b[0m\n",
            " \u001b[36mtest_requests.py\u001b[0m::test_good_urls[201]\u001b[0m \u001b[32m✓\u001b[0m                          \u001b[32m18% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▊        \u001b[0m\n",
            " \u001b[36mtest_requests.py\u001b[0m::test_good_urls[202]\u001b[0m \u001b[32m✓\u001b[0m                          \u001b[32m27% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▊       \u001b[0m\n",
            " \u001b[36mtest_requests.py\u001b[0m::test_good_urls[203]\u001b[0m \u001b[32m✓\u001b[0m                          \u001b[32m36% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▋      \u001b[0m\n",
            " \u001b[36mtest_requests.py\u001b[0m::test_good_urls[204]\u001b[0m \u001b[32m✓\u001b[0m                          \u001b[32m45% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▋     \u001b[0m\n",
            " \u001b[36mtest_requests.py\u001b[0m::test_good_urls[303]\u001b[0m \u001b[32m✓\u001b[0m                          \u001b[32m55% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▌    \u001b[0m\n",
            " \u001b[36mtest_requests.py\u001b[0m::test_good_urls[304]\u001b[0m \u001b[32m✓\u001b[0m                          \u001b[32m64% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▍   \u001b[0m\n",
            " \u001b[36mtest_requests.py\u001b[0m::test_bad_urls[401]\u001b[0m \u001b[32m✓\u001b[0m                           \u001b[32m73% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▍  \u001b[0m\n",
            " \u001b[36mtest_requests.py\u001b[0m::test_bad_urls[404]\u001b[0m \u001b[32m✓\u001b[0m                           \u001b[32m82% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▎ \u001b[0m\n",
            " \u001b[36mtest_requests.py\u001b[0m::test_bad_urls[500]\u001b[0m \u001b[32m✓\u001b[0m                           \u001b[32m91% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▏\u001b[0m\n",
            " \u001b[36mtest_requests.py\u001b[0m::test_bad_urls[501]\u001b[0m \u001b[32m✓\u001b[0m                          \u001b[32m100% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\n",
            "\n",
            "Results (2.12s):\n",
            "\u001b[32m      11 passed\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RW4Vq1ONjktF"
      },
      "source": [
        "# running tests in parallel\n",
        "\n",
        "[Reference](https://pypi.org/project/pytest-xdist/)\n",
        "\n",
        "The `pytest-xdist` plugin extends pytest with some unique test execution modes:\n",
        "\n",
        "- **test run parallelization**: if you have multiple CPUs or hosts you can use those for a combined test run. This allows to speed up development or to use special resources of remote machines.\n",
        "- **--looponfail**: run your tests repeatedly in a subprocess. After each run pytest waits until a file in your project changes and then re-runs the previously failing tests. This is repeated until all tests pass after which again a full run is performed.\n",
        "- **Multi-Platform coverage**: you can specify different Python interpreters or different platforms and run tests in parallel on all of them.\n",
        "- **--boxed** and **pytest-forked**: running each test in its own process, so that if a test catastrophically crashes, it doesn't interfere with other tests\n",
        "\n",
        "We're going to cover only test run parallelization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1ur1uHMv29W"
      },
      "source": [
        "first, lets install `pytest-xdist`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwNCMTHhvpDw"
      },
      "source": [
        "pip install -qq pytest-xdist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6sd_3BXv63z"
      },
      "source": [
        "now, lets write a few long running tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UygEsQ23vy0w",
        "outputId": "fd7ee9b2-c70e-4398-d099-e4c770bbb48b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%file test_parallel.py\n",
        "\n",
        "import time\n",
        "def test_t1():\n",
        "    time.sleep(2)\n",
        "\n",
        "def test_t2():\n",
        "    time.sleep(2)\n",
        "\n",
        "def test_t3():\n",
        "    time.sleep(2)\n",
        "\n",
        "def test_t4():\n",
        "    time.sleep(2)\n",
        "\n",
        "def test_t5():\n",
        "    time.sleep(2)\n",
        "\n",
        "def test_t6():\n",
        "    time.sleep(2)\n",
        "\n",
        "def test_t7():\n",
        "    time.sleep(2)\n",
        "\n",
        "def test_t8():\n",
        "    time.sleep(2)\n",
        "\n",
        "def test_t9():\n",
        "    time.sleep(2)\n",
        "\n",
        "def test_t10():\n",
        "    time.sleep(2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test_parallel.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI-BoKN-xNhl"
      },
      "source": [
        "now, we can run these tests in parallel using the `pytest -n NUM` commandline parameter.\n",
        "\n",
        "Lets use 10 threads, this will allow us to finish in 2 seconds rather than 20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GfcAn2twcrl",
        "outputId": "bad9a76a-74c4-4ccf-967e-07cae9ba4866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "!python -m pytest -n 10 test_parallel.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.6.9, pytest-5.3.5, py-1.8.1, pluggy-0.13.1\n",
            "rootdir: /content\n",
            "plugins: xdist-1.31.0, forked-1.1.3, timeout-1.3.4\n",
            "gw0 [10] / gw1 [10] / gw2 [10] / gw3 [10] / gw4 [10] / gw5 [10] / gw6 [10] / gw7 [10] / gw8 [10] / gw9 [10]\u001b[0m\n",
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                               [100%]\u001b[0m\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m10 passed\u001b[0m\u001b[32m in 5.94s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0AYxisCVDHa"
      },
      "source": [
        "# Codebase to test: class Person\n",
        "\n",
        "Lets reuse the `Person` and `OlympicRunner` classes we've defined in earlier chapters in order to see how to write tests\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoLNB6529y7l",
        "outputId": "5966f5b4-5090-4d43-ea08-6744455b48d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%file person.py\n",
        "\n",
        "# Person v1\n",
        "class Person:\n",
        "    def __init__(self, name):\n",
        "        name = name\n",
        "    def __repr__(self):\n",
        "        return f\"{type(self).__name__}({self.name!r})\"\n",
        "    def walk(self):\n",
        "        print(self.name, 'walking')\n",
        "    def run(self):\n",
        "        print(self.name,'running')\n",
        "    def swim(self):\n",
        "        print(self.name,'swimming')\n",
        "\n",
        "class OlympicRunner(Person):\n",
        "    def run(self):\n",
        "        print(self.name,self.name,\"running incredibly fast!\")\n",
        "\n",
        "    def show_medals(self):\n",
        "        print(self.name, 'showing my olympic medals')\n",
        "\n",
        "def train(person):\n",
        "    person.walk()\n",
        "    person.swim()\n",
        "    person.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting person.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbPMcdlA-spP"
      },
      "source": [
        "# our first test\n",
        "\n",
        "- [conventions](https://docs.pytest.org/en/latest/goodpractices.html#conventions-for-python-test-discovery)\n",
        "  1. files with tests should be called `test_*.py` or `*_test.py `\n",
        "  2. test function name should start with `test_`\n",
        "\n",
        "- to see if our code works, we can use the `assert` python keyword. pytest adds hooks to assertions to make them more useful"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeURkKHH-qIk",
        "outputId": "5614f16a-dc9a-42a3-d486-4e7a57af3cd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%file test_person1.py\n",
        "from person import Person\n",
        "\n",
        "# our first test\n",
        "def test_preson_name():\n",
        "    terry = Person('Terry Gilliam')\n",
        "    assert terry.name == 'Terry Gilliam'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test_person1.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7vECkgGL5Y9",
        "outputId": "840e30da-f11b-4ff4-cd64-064141fe9099",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "source": [
        "!python -m pytest"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.6.9, pytest-3.6.4, py-1.8.1, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "collected 1 item                                                               \u001b[0m\u001b[1m\n",
            "\n",
            "test_person1.py F\u001b[36m                                                        [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[31m\u001b[1m_______________________________ test_preson_name _______________________________\u001b[0m\n",
            "\n",
            "\u001b[1m    def test_preson_name():\u001b[0m\n",
            "\u001b[1m        terry = Person('Terry Gilliam')\u001b[0m\n",
            "\u001b[1m>       assert terry.name == 'Terry Gilliam'\u001b[0m\n",
            "\u001b[1m\u001b[31mE       AttributeError: 'Person' object has no attribute 'name'\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_person1.py\u001b[0m:6: AttributeError\n",
            "\u001b[31m\u001b[1m=========================== 1 failed in 0.03 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiicargLAClz"
      },
      "source": [
        "## lets run our tests\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8F4LBwwAFLf",
        "outputId": "8dbd169b-6f3a-4c66-bee5-95a6174237e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# execute the tests via pytest, arguments are passed to pytest\n",
        "ipytest.run('-qq')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ERROR: file not found: adv python 08 - test driven development.ipynb\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAhNkTlV_yXT"
      },
      "source": [
        "## running our first test\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "661ExV-V9y7p"
      },
      "source": [
        "# very simple test\n",
        "def test_person_repr1():\n",
        "    assert str(Person('terry gilliam')) == f\"Person('terry gilliam')\"\n",
        "\n",
        "# test using mock object\n",
        "def test_train1():\n",
        "    person = mocking.Mock()\n",
        "\n",
        "    train(person)\n",
        "    person.walk.assert_called_once()\n",
        "    person.run.assert_called_once()\n",
        "    person.swim.assert_called_once()\n",
        "\n",
        "# create factory for person's name\n",
        "@pytest.fixture\n",
        "def person_name():\n",
        "    return 'terry gilliam'\n",
        "\n",
        "# create factory for Person, that requires a person_name\n",
        "@pytest.fixture\n",
        "def person(person_name):\n",
        "    return Person(person_name)\n",
        "\n",
        "# test using mock object\n",
        "def test_train2(person):\n",
        "    # this makes sure no other method is called\n",
        "    person = mocking.create_autospec(person)\n",
        "\n",
        "    train(person)\n",
        "    person.walk.assert_called_once()\n",
        "    person.run.assert_called_once()\n",
        "    person.swim.assert_called_once()\n",
        "\n",
        "\n",
        "# test Person using and request a person, person_name from the fixtures\n",
        "def test_person_repr2(person, person_name):\n",
        "    assert str(person) == f\"Person('{person_name}')\"\n",
        "\n",
        "# fixture with multiple values\n",
        "@pytest.fixture(params=['usain bolt', 'Matthew Wells'])\n",
        "def olympic_runner_name(request):\n",
        "    return request.param\n",
        "\n",
        "@pytest.fixture\n",
        "def olympic_runner(olympic_runner_name):\n",
        "    return OlympicRunner(olympic_runner_name)\n",
        "\n",
        "# test train() using mock object for print\n",
        "@mocking.patch('builtins.print')\n",
        "def test_train3(mocked_print, olympic_runner):\n",
        "    train(olympic_runner)\n",
        "    mocked_print.assert_called()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV8jU6b29y7t",
        "outputId": "0d737fdc-a703-4003-8cde-2a9be68d7dc9"
      },
      "source": [
        "# execute the tests via pytest, arguments are passed to pytest\n",
        "ipytest.run('-qq')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "......                                                                                                           [100%]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}